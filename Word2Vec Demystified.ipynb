{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We start with the motivating example:\n",
    "\n",
    "## Word Vectors\n",
    "\n",
    "### Synonyms\n",
    "\n",
    "$$ \\vec {happiness} \\approx \\vec {joy} $$\n",
    "\n",
    "$$ \\vec {dog} \\approx \\vec {puppy} $$\n",
    "\n",
    "And a seemingly weird example that will make sense to you later:\n",
    "\n",
    "$$ \\vec {one} \\approx \\vec {two} $$\n",
    "\n",
    "### Analogies\n",
    "\n",
    "$$ \\vec {man} + \\vec {woman} - \\vec {king} \\approx \\vec {queen} $$\n",
    "\n",
    "$$ \\vec {Berlin} + \\vec {Germany} - \\vec {France} \\approx \\vec {Paris} $$\n",
    "\n",
    "### Visualization\n",
    "\n",
    "** INSERT VISUALIZATION HERE **\n",
    "\n",
    "## Magic? Nope!\n",
    "\n",
    "Does this seem like magic to you? It might, but Word2Vec is learning what we call 'embeddings' for words, and embeddings are actually quite simple.\n",
    "From the analogy above, you might conclude that Word2Vec is learning how to do analogies. That's actually *not* the case!\n",
    "\n",
    "### What is being trained?\n",
    "\n",
    "The model instead examines a large body of text and models how words occur together. Consider the meaning of this assertion:\n",
    "\n",
    "** You shall know a word by its neighbors. **\n",
    "\n",
    "Put another way, the semantic content of a word arises from its usage, which is best measured by other words it is seen near.\n",
    "\n",
    "### Why does it work?\n",
    "\n",
    "Different words occur together in different contexts. When we model words in *vector space*, we make the vectors point in similar directions based on how those words occur together. Properties naturally manifest like the ones you see above.\n",
    "\n",
    "\n",
    "## Boring Notation\n",
    "\n",
    "**Notation is important!** The ability to formalize and express these concepts in a structured way helps you *truly* grasp the relationship between concepts you *think* you know and form connections to new ones.\n",
    "\n",
    "$V$ is a matrix\n",
    "\n",
    "$\\vec v_c$ is the $c^{th}$ column vector taken from matrix $V$\n",
    "\n",
    "$y_i$ is the $i^{th}$ scalar element of the vector $y$\n",
    "\n",
    "$x$ is a vector\n",
    "\n",
    "|W| is the length of the vocabulary\n",
    "\n",
    "$v^Tu$ is the dot product of $v$ and $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "Often used interchangeably, the terms *word vectors* and *word embeddings* refer to a vector representation of a word with some (learned) semantic meaning.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Some questions that will be answered over the next few sections:\n",
    "\n",
    "* What is an embedding?\n",
    "* Why do we want good representations?\n",
    "* Why are embeddings considered good representations?\n",
    "\n",
    "## Representations\n",
    "\n",
    "It's often said that deep learning is about learning good representations. In traditional machine learning fields, representations were often concocted by hand by domain experts and researchers. **We'll see later why good representations matter, and what makes a representation good.**\n",
    "\n",
    "Say you have a million words in your vocabulary and a machine learning model has one of those words as an input. How would you represent the word as a vector?\n",
    "\n",
    "## One-Hot Vectors\n",
    "\n",
    "Traditionally, you would have a vector with 1 million components, or dimensions, and you would set the value of every dimension to $0$ except the dimension corresponding to the word in question (based on an index), which you would set to one. We call this **one-hot**.\n",
    "\n",
    "Let's be *formal* and use some mathematical notation:\n",
    "\n",
    "*Formally*, the one-hot vector for the word of index $i$:\n",
    "\n",
    "$$ x = \\begin{bmatrix} 0 & 0 & ... & 1  & ... & 0  \\end{bmatrix} $$\n",
    "\n",
    "Where the $i^{th}$ element is 1 and all other elements are 0.\n",
    "\n",
    "## Dense representation\n",
    "\n",
    "In contrast to one-hot vectors, which are sparse (contain many 0s), Word2Vec trains dense vectors, which contain no 0s and are of lower dimension (say 100 - 300). The vector for every word is different; every word vector points in a different direction. These dense vectors are collected in column form to make an **embedding matrix**.\n",
    "\n",
    "### Formulation\n",
    "\n",
    "*Formally*\n",
    "\n",
    "The **dense representation** of a one-hot encoded word $x$ that represents the $c^{th}$ word in the vocabulary is:\n",
    "\n",
    "$$ V x =  \\vec v_c $$\n",
    "\n",
    "Where $V$ is the embedding matrix\n",
    "\n",
    "And $\\vec v_c$ is a vector in embedding space\n",
    "\n",
    "### Explanation\n",
    "\n",
    "This looks like matrix-vector multiplication, but it's actually even simpler. If you remember you matrix-vector multiplication rules, a matrix $V$ times a vector $x$ is a linear combination of all columns in matrix $V$ by every element of vector $x$. In other words, for every column in $V$, multiply the column vector $\\vec v_i$ by the scalar $x_i$ and add up all $v_i$s.\n",
    "\n",
    "$$ Vx = \\sum_i^{|W|} x_i \\vec v_i $$\n",
    "\n",
    "Since $x$ is one-hot, only one column is multiplied by a non-zero element. This is actually just selecting column $i$ from matrix $V$. $V$ is *precisely* a collection of word vectors in column form.\n",
    "\n",
    "### Examples\n",
    "\n",
    "To embed the one-hot vector $x$ for word $c$ from embedding matrix $V$:\n",
    "\n",
    "$$ Vx = \\vec v_c $$\n",
    "\n",
    "To embed the one-hot vector $y$ for word $o$ from embedding matrix $U$:\n",
    "\n",
    "$$ Uy = \\vec u_o $$\n",
    "\n",
    "## Answers\n",
    "\n",
    "What is an embedding?\n",
    "\n",
    "** A dense vector representation of a word **\n",
    "\n",
    "** A matrix with $|W|$ columns of dense vectors **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Properties\n",
    "\n",
    "We have a collection of word vectors for words in our vocabulary - how do we know if they are good representations?\n",
    "\n",
    "It must be the case that a **good word representation** have certain properties. Let's first look at what those properties are and later on demonstrate how those properties can be achieved.\n",
    "\n",
    "Recall from linear algebra that two vectors of the same dimension can be combined in two ways:\n",
    "\n",
    "### Vector Sum\n",
    "\n",
    "The sum of vectors $v$ and $u$ is defined as addition of their individual elements.\n",
    "\n",
    "$$ w_i = v_i + u_i $$\n",
    "\n",
    "The resulting vector is the equivalent of adding drawing the tail of $u$ from the head of $v$:\n",
    "\n",
    "** Visualization **\n",
    "\n",
    "### Dot Product\n",
    "\n",
    "The dot product of vectors $v$ and $u$ is defined as the sum of the product of their individual elements.\n",
    "\n",
    "$$ v^T u = \\sum_i v_i * u_i $$\n",
    "\n",
    "#### Similarity\n",
    "\n",
    "The dot product is proportional to the similarity of 2 vectors:\n",
    "\n",
    "Vectors that point in the same direction have a high dot product.\n",
    "Vectors that point in opposite directions have a low dot product.\n",
    "Vectors that point in unrelated (orthogonal) directions have 0 dot product.\n",
    "\n",
    "### Word2Vec vs One-Hot\n",
    "\n",
    "One-hot vectors have 0 dot product. This implies that all words are equally unrelated to each other. ** This is neither true nor desirable **\n",
    "\n",
    "Dense embeddings have high dot products for words that appear together and low dot products for words that do not. If we believe that similar & related words appear together, then we'd like them to point in similar directions and thus have a high dot product.\n",
    "\n",
    "### Good Representation\n",
    "\n",
    "A good representation encodes as much relevant information as possible. It accurately depicts prior knowledge we have about the data, and helps us create a model for the underlying task that takes advantage of that prior knowledge.\n",
    "\n",
    "We know that words are related to each other in varying degrees, and along varying axis.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "* The relationship between \"king\" and \"man\" is along the axis of specificity\n",
    "* The relationship between \"man\" and \"woman\" is along the axis of gender\n",
    "* The relationship between \"one\" and \"two\" is along the axis of plurality\n",
    "\n",
    "Dense embeddings represent the relatedness of words, whereas one-hot encodings do not. A downstream machine learning task can make use of this insight and will not have to re-learn how \"man\" and \"king\" are related. When a good representation is learned, the overall task is becomes easier.\n",
    "\n",
    "### Visualize\n",
    "\n",
    "** Insert Visualization AGAIN**\n",
    "\n",
    "** Point out how vectors are arranged **\n",
    "\n",
    "## Questions\n",
    "\n",
    "Why are good representations useful?\n",
    "\n",
    "** Good representations take advantage of prior knowledge to make the underlying task easier **\n",
    "\n",
    "Why are dense embeddings good representations?\n",
    "\n",
    "** Dense embeddings represent words based on their relatedness, so that a downstream task can make use of such relatedness **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic\n",
    "\n",
    "### Example text:\n",
    "\n",
    "Berlin is a city in East Germany\n",
    "\n",
    "+ so we want a classifier ...\n",
    "+ the dot product is in some arbitrary scale, so the score doesn't sum to one and is merely relative\n",
    "+ if we want probabilities, we exponentiate the score normalize over all scores to sum to 1\n",
    "+ the score is also known as an \"unnormalized log probability\"\n",
    "$$ P(O,C) = \\exp( \\mathrm{score} ) $$\n",
    "\n",
    "and we want to maximize\n",
    "$$ P(O|C) = P(O,C) / \\sum P(O', C) = P(O,C) / P(C) $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "### Random initialize - we make no assumptions\n",
    "\n",
    "### Formulation of objective function\n",
    "\n",
    "BLACK BOX OPTIMIZATION \n",
    "\n",
    "OR\n",
    "\n",
    "Detailed ->\n",
    "$ -log(p(o|c)) $ -> $ \\hat y - y$\n",
    "\n",
    "\n",
    "$$ v_c = v_c - \\alpha \\frac {\\partial L}{\\partial v_c}$$\n",
    "\n",
    "$$ \\frac {\\partial L}{\\partial v_c} = \\sum_{j \\ne k}^{W} \\hat y_j * u_{i,j}  -  u_k $$\n",
    "\n",
    "### Linear Algebra intuition for gradient update\n",
    "\n",
    "Write out derivatives on your own time!\n",
    "\n",
    "Here are the gradients already worked out:\n",
    "* Gradient wrt input vector, output vectors*\n",
    "\n",
    "In backpropogation, when I do an update, I subtract the gradient. When I add a gradient to a vector, I'm pushing that vector towards the gradient. (show diagram)\n",
    "\n",
    "Check out the gradients for $v_c$ and $u_o$.\n",
    "\n",
    "### Visualize\n",
    "\n",
    "How do related word vectors move towards each other in 3D space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking forward\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "When you have complicated models, you have more transformations to latent spaces.\n",
    "However, the intuition remains the same; you're dealing with dot products - similarities (negative distances).\n",
    "\n",
    "## Bonus Material\n",
    "\n",
    "L2 distance between 2 word vectors \n",
    "$$ d^2 = (x - z)^2 = x^2 + z^2 - 2xz $$\n",
    "\n",
    "Setting them to normal vectors\n",
    "$$ d^2 = 2 - 2xz $$\n",
    "\n",
    "Then exponentiating for unnormalized probability ->\n",
    "\n",
    "$$ exp(-d^2) $$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
