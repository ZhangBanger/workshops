{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We start with the motivating example:\n",
    "\n",
    "## Word Vectors\n",
    "\n",
    "$$ \\vec {man} + \\vec {woman} - \\vec {king} \\approx \\vec {queen} $$\n",
    "\n",
    "$$ \\vec {Berlin} + \\vec {Germany} - \\vec {France} \\approx \\vec {Paris} $$\n",
    "\n",
    "### Visualization\n",
    "\n",
    "** INSERT VISUALIZATION HERE **\n",
    "\n",
    "## Magic?\n",
    "\n",
    "Does this seem like magic to you? It might, but Word2Vec is learning what we call 'embeddings' for words, and embeddings are actually quite simple.\n",
    "From the analogy above, you might conclude that Word2Vec is learning how to do analogies. That's actually *not* the case!\n",
    "\n",
    "### What is being trained?\n",
    "\n",
    "The model instead examines a large body of text and models how words co-occur. The significance of co-occurence stems from the insight in this phrase:\n",
    "\n",
    "You shall know a word by its neighbors.\n",
    "\n",
    "### Why does it work?\n",
    "\n",
    "Different words occur together in different frequencies, and in *vector space*, properties manifest like the ones you see above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "Often used interchangeably, the terms *word vectors* and *word embeddings* mean something very specific. Here is the setup.\n",
    "\n",
    "## One-Hot Vectors\n",
    "\n",
    "Say you have a million words in your vocabulary. Now say you have a machine learning model has one of those words as an input. How would you represent the word as a vector?\n",
    "\n",
    "Traditionally, you would have a vector with 1 million components, or dimensions, and you would set the value of every dimension to $0$ except the dimension corresponding to the word in question (based on an index), which you would set to one.\n",
    "\n",
    "The vector looks something like\n",
    "\n",
    "$$ \\begin{bmatrix} 0 & 0 & 0 & ... & 1  & ... & 0  \\end{bmatrix} $$\n",
    "\n",
    "## Dense representation\n",
    "\n",
    "+ vocabulary is one-hot, which is one for a word and zero otherwise\n",
    "\n",
    "      V c -> v_c\n",
    "      W o -> w_o\n",
    "      \n",
    "## Linear Algebra\n",
    "\n",
    "### Dot Product\n",
    "\n",
    "### Orthogonality\n",
    "\n",
    "## Questions\n",
    "\n",
    "* Why are one-hot vectors bad?\n",
    "* Why are dense embeddings good?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Linear algebra \n",
    "+ latent semantic space -> score\n",
    "\n",
    "cosine similarity <-> how close the vectors are, unit circle diagram\n",
    "\n",
    "     $$\\mathrm{score} = v_c \\cdot w_o$$\n",
    "\n",
    "+ we need this, because in one-hot everything will be _orthogonal_ (x*y = 0), so this way we getting a more tractable space. orthogonal -> 90degrees.\n",
    "\n",
    "Write the update rule\n",
    "\n",
    "$$ v_c = v_c - \\alpha \\frac {\\partial L}{\\partial v_c}$$\n",
    "\n",
    "$$ = \\sum_{j \\ne k}^{W} \\hat y_j * u_{i,j} $$\n",
    "\n",
    "## Probabilistic\n",
    "\n",
    "### Example text:\n",
    "\n",
    "Berlin is a city in East Germany\n",
    "\n",
    "+ so we want a classifier ...\n",
    "+ the dot product is in some arbitrary scale, so the score doesn't sum to one and is merely relative\n",
    "+ if we want probabilities, we exponentiate the score normalize over all scores to sum to 1\n",
    "+ the score is also known as an \"unnormalized log probability\"\n",
    "$$ P(O,C) = \\exp( \\mathrm{score} ) $$\n",
    "\n",
    "and we want to maximize\n",
    "$$ P(O|C) = P(O,C) / \\sum P(O', C) = P(O,C) / P(C) $$\n",
    "\n",
    "## Gradients\n",
    "\n",
    "### Random initialize - we make no assumptions\n",
    "\n",
    "### Formulation of objective function\n",
    "\n",
    "BLACK BOX OPTIMIZATION \n",
    "\n",
    "OR\n",
    "\n",
    "Detailed ->\n",
    "$ -log(p(o|c)) $ -> $ \\hat y - y$\n",
    "\n",
    "### Linear Algebra intuition for gradient update\n",
    "\n",
    "Write out derivatives on your own time!\n",
    "\n",
    "Here are the gradients already worked out:\n",
    "* Gradient wrt input vector, output vectors*\n",
    "\n",
    "In backpropogation, when I do an update, I subtract the gradient. When I add a gradient to a vector, I'm pushing that vector towards the gradient. (show diagram)\n",
    "\n",
    "Check out the gradients for $v_c$ and $u_o$.\n",
    "\n",
    "### Visualize\n",
    "\n",
    "How do related word vectors move towards each other in 3D space?\n",
    "\n",
    "\n",
    "### Bonus\n",
    "\n",
    "L2 distance between 2 word vectors \n",
    "$$ d^2 = (x - z)^2 = x^2 + z^2 - 2xz $$\n",
    "\n",
    "Setting them to normal vectors\n",
    "$$ d^2 = 2 - 2xz $$\n",
    "\n",
    "Then exponentiating for unnormalized probability ->\n",
    "\n",
    "$$ exp(-d^2) $$\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "When you have complicated models, you have more transformations to latent spaces.\n",
    "However, the intuition remains the same; you're dealing with dot products - similarities (negative distances)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
