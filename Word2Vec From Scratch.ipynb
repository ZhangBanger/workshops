{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case\n",
    "\n",
    "## Context:\n",
    "   Berlin is a city in East Germany\n",
    "\n",
    "-> _unsupervised_ setup, but eventually we get a _supervised_ classification task\n",
    "\n",
    "-> we want a _classifier_ (probability of a class) that predicts ...\n",
    "\n",
    "## Embedding space:\n",
    "\n",
    "+ vocabulary is one-hot, which is one for a word and zero otherwise\n",
    "\n",
    "      V c -> v_c\n",
    "      W o -> w_o\n",
    " \n",
    "## Linear algebra \n",
    "+ latent semantic space -> score\n",
    "\n",
    "cosine similarity <-> how close the vectors are, unit circle diagram\n",
    "\n",
    "     $$\\mathrm{score} = v_c \\cdot w_o$$\n",
    "\n",
    "+ we need this, because in one-hot everything will be _orthogonal_ (x*y = 0), so this way we getting a more tractable space. orthogonal -> 90degrees.\n",
    "\n",
    "## Probabilistic\n",
    "+ so we want a classifier ...\n",
    "+ the dot product is in some arbitrary scale, so the score doesn't sum to one and is merely relative\n",
    "+ if we want probabilities, we exponentiate the score normalize over all scores to sum to 1\n",
    "+ the score is also known as an \"unnormalized log probability\"\n",
    "$$ P(O,C) = \\exp( \\mathrm{score} ) $$\n",
    "\n",
    "and we want to maximize\n",
    "$$ P(O|C) = P(O,C) / \\sum P(O', C) = P(O,C) / P(C) $$\n",
    "\n",
    "## Gradients\n",
    "\n",
    "### Element-wise derivative\n",
    "\n",
    "### Matrix-vector visualization of backprop\n",
    "\n",
    "### Linear Algebra intuition\n",
    "\n",
    "In backpropogation, when I do an update, I subtract the gradient. When I add a gradient to a vector, I'm pushing that vector towards the gradient. (show diagram)\n",
    "\n",
    "Check out the gradients for $v_c$ and $u_o$.\n",
    "\n",
    "\n",
    "### Bonus\n",
    "\n",
    "L2 distance between 2 word vectors \n",
    "$$ d^2 = (x - z)^2 = x^2 + z^2 - 2xz $$\n",
    "\n",
    "Setting them to normal vectors\n",
    "$$ d^2 = 2 - 2xz $$\n",
    "\n",
    "Then exponentiating for unnormalized probability ->\n",
    "\n",
    "$$ exp(-d^2) $$\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "When you have complicated models, you have more transformations to latent spaces.\n",
    "However, the intuition remains the same; you're dealing with dot products - similarities (negative distances)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
