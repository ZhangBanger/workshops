{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We start with the motivating example:\n",
    "\n",
    "## Word Vectors\n",
    "\n",
    "### Synonyms\n",
    "\n",
    "$$ \\vec {happiness} \\approx \\vec {joy} $$\n",
    "\n",
    "$$ \\vec {dog} \\approx \\vec {puppy} $$\n",
    "\n",
    "And a seemingly weird example that will make sense to you later:\n",
    "\n",
    "$$ \\vec {one} \\approx \\vec {two} $$\n",
    "\n",
    "### Analogies\n",
    "\n",
    "You can do arithmetic and get analogies!\n",
    "\n",
    "$$ \\vec {king} - \\vec {man} + \\vec {woman} \\approx \\vec {queen} $$\n",
    "\n",
    "$$ \\vec {Berlin} - \\vec {Germany} + \\vec {France} \\approx \\vec {Paris} $$\n",
    "\n",
    "** INSERT VISUALIZATION HERE **\n",
    "\n",
    "## Magic? Nope!\n",
    "\n",
    "Does this seem like magic to you? It might, but Word2Vec is learning what we call 'embeddings' for words, and embeddings are actually quite simple.\n",
    "From the examples above, you might conclude that Word2Vec understands what an analogy is, as well knowing that Berlin is a city and Germany is a country. That's actually *not* the case!\n",
    "\n",
    "\n",
    "\n",
    "## Boring Notation\n",
    "\n",
    "**Notation is important!** The ability to formalize and express these concepts in a structured way helps you *truly* grasp the relationship between concepts you *think* you know and form connections to new ones.\n",
    "\n",
    "$V$ is a matrix\n",
    "\n",
    "$\\vec v_c$ is the $c^{th}$ column vector taken from matrix $V$\n",
    "\n",
    "$y_i$ is the $i^{th}$ scalar element of the vector $y$\n",
    "\n",
    "$x$ is a vector\n",
    "\n",
    "|W| is the length of the vocabulary\n",
    "\n",
    "$v^Tu$ is the dot product of $v$ and $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "Often used interchangeably, the terms *word vectors* and *word embeddings* refer to a vector representation of a word with some (learned) semantic meaning.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Some questions that will be answered over the next few sections:\n",
    "\n",
    "* What is an embedding?\n",
    "* What is a good representation?\n",
    "* Why are embeddings considered good representations?\n",
    "\n",
    "## Representations\n",
    "\n",
    "It's often said that deep learning is about learning good representations. In traditional machine learning fields, representations were often concocted by hand by domain experts and researchers. **We'll see later why good representations matter, and what makes a representation good.**\n",
    "\n",
    "Say you have a million words in your vocabulary and a machine learning model has one of those words as an input. How would you represent the word as a vector?\n",
    "\n",
    "## One-Hot Vectors\n",
    "\n",
    "Traditionally, you would have a vector with 1 million components, or dimensions, and you would set the value of every dimension to $0$ except the dimension corresponding to the word in question (based on an index), which you would set to one. We call this **one-hot**.\n",
    "\n",
    "Let's be *formal* and use some mathematical notation:\n",
    "\n",
    "*Formally*, the one-hot vector for the word of index $i$:\n",
    "\n",
    "$$ x = \\begin{bmatrix} 0 & 0 & ... & 1  & ... & 0  \\end{bmatrix} $$\n",
    "\n",
    "Where the $i^{th}$ element is 1 and all other elements are 0.\n",
    "\n",
    "## Dense representation\n",
    "\n",
    "In contrast to one-hot vectors, which are sparse (contain many 0s), Word2Vec trains dense vectors, which contain no 0s and are of lower dimension (say 100 - 300), significantly smaller than the vocabulary. The vector for every word is different; every word vector points in a different direction. These dense vectors are collected in column form to make an **embedding matrix**.\n",
    "\n",
    "### Formulation\n",
    "\n",
    "*Formally*\n",
    "\n",
    "The **dense representation** of a one-hot encoded word $x$ that represents the $c^{th}$ word in the vocabulary is:\n",
    "\n",
    "$$ V x =  \\vec v_c $$\n",
    "\n",
    "Where $V$ is the embedding matrix\n",
    "\n",
    "And $\\vec v_c$ is a vector in embedding space\n",
    "\n",
    "### Explanation\n",
    "\n",
    "This looks like matrix-vector multiplication, but it's actually even simpler. If you remember you matrix-vector multiplication rules, a matrix $V$ times a vector $x$ is a linear combination of all columns in matrix $V$ by every element of vector $x$. In other words, for every column in $V$, multiply the column vector $\\vec v_i$ by the scalar $x_i$ and add up all $v_i$s.\n",
    "\n",
    "$$ Vx = \\sum_i^{|W|} x_i \\vec v_i $$\n",
    "\n",
    "Since $x$ is one-hot, only one column is multiplied by a non-zero element. This is actually just selecting column $i$ from matrix $V$. $V$ is *precisely* a collection of word vectors in column form.\n",
    "\n",
    "### Examples\n",
    "\n",
    "To embed the one-hot vector $x$ for word $c$ from embedding matrix $V$:\n",
    "\n",
    "$$ Vx = \\vec v_c $$\n",
    "\n",
    "To embed the one-hot vector $y$ for word $o$ from embedding matrix $U$:\n",
    "\n",
    "$$ Uy = \\vec u_o $$\n",
    "\n",
    "## Answers\n",
    "\n",
    "What is an embedding?\n",
    "\n",
    "** A dense vector representation of a word **\n",
    "\n",
    "** A matrix with $|W|$ columns of dense vectors **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model\n",
    "\n",
    "We have a collection of word vectors for words in our vocabulary - how do we know if they are good representations?\n",
    "\n",
    "It must be the case that a **good word representation** have certain properties. Let's first look at what those properties are and later on demonstrate how those properties can be achieved.\n",
    "\n",
    "Recall from linear algebra that two vectors of the same dimension can be combined in two ways:\n",
    "\n",
    "### Vector Sum\n",
    "\n",
    "The sum of vectors $v$ and $u$ is defined as addition of their individual elements.\n",
    "\n",
    "$$ w_i = v_i + u_i $$\n",
    "\n",
    "The resulting vector is the equivalent of adding drawing the tail of $u$ from the head of $v$:\n",
    "\n",
    "### Dot Product\n",
    "\n",
    "The dot product of vectors $\\vec v_c$ and $\\vec u_o$ is defined as the sum of the product of their individual elements.\n",
    "\n",
    "$$ \\vec v_c^T \\vec u_o = \\sum_i v_i * u_i $$\n",
    "\n",
    "#### Similarity\n",
    "\n",
    "The dot product is proportional to the similarity of 2 vectors:\n",
    "\n",
    "Vectors that point in the same direction have a high dot product.\n",
    "Vectors that point in opposite directions have a low dot product.\n",
    "Vectors that point in unrelated (orthogonal) directions have 0 dot product.\n",
    "\n",
    "### Word2Vec vs One-Hot\n",
    "\n",
    "One-hot vectors have 0 dot product. This implies that all words are equally unrelated to each other. ** This is neither true nor desirable **\n",
    "\n",
    "Dense embeddings have high dot products for words that appear together and low dot products for words that do not. If we believe that similar & related words appear together, then we'd like them to point in similar directions and thus have a high dot product.\n",
    "\n",
    "### Good Representation\n",
    "\n",
    "**A good representation encodes as much relevant information as possible. It accurately depicts prior knowledge we have about the data, and helps us create a model for the underlying task that takes advantage of that prior knowledge.**\n",
    "\n",
    "For example, with a task like question-answering, we'd like to not have to learn word relationships from scratch. When we have access to a large dataset of words that we can train embeddings on, but a comparatively smaller dataset to train on our task, we'd like our task to use as much outside knowledge as possible.\n",
    "\n",
    "We know that words are related to each other in along different axis.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "* The relationship between \"king\" and \"man\" is along the axis of instantiation - a king is an instance of a man\n",
    "* The relationship between \"man\" and \"woman\" is along the axis of gender\n",
    "* The relationship between \"one\" and \"two\" is along the axis of plurality - two is a multiple of one\n",
    "\n",
    "Dense embeddings represent the relatedness of words, whereas one-hot encodings do not. A downstream machine learning task can make use of this insight and will not have to re-learn how \"man\" and \"king\" are related. When a good representation is learned, the overall task is becomes easier.\n",
    "\n",
    "### Visualize in 3D\n",
    "\n",
    "** Insert Visualization AGAIN**\n",
    "\n",
    "** Point out how vectors are arranged **\n",
    "\n",
    "Note that $\\vec{man}$, $\\vec{woman}$, $\\vec{king}$, and $\\vec{queen}$ all point in the same general direction.\n",
    "\n",
    "Also note that $\\vec{man}$ and $\\vec{woman}$ line up with each other in the same way that $\\vec{king}$ and $\\vec{queen}$ do.\n",
    "\n",
    "** Does this comport with your intuition about their relatedness? **\n",
    "\n",
    "## Answers\n",
    "\n",
    "What is a good representation?\n",
    "\n",
    "** Good representations take advantage of prior knowledge to make a downstream machine learning task easier **\n",
    "\n",
    "Why are dense embeddings good representations?\n",
    "\n",
    "** Dense embeddings represent words based on their relatedness, so that a downstream task can make use of such relatedness without having to learn it on its own. The dot product of two related words is high, and the dot product of two unrelated words is low. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Word2Vec Model\n",
    "\n",
    "** You shall know a word by its neighbors. **\n",
    "\n",
    "Word2Vec trains a probabilistlic model to maximize the probability of seeing a neighbor word given an input word from a given training corpus.\n",
    "\n",
    "## Questions\n",
    "\n",
    "* What is the Word2Vec model trained on?\n",
    "* How does Word2Vec update individual word vectors when training?\n",
    "* Why do those updates lead to vector space properties discussed above?\n",
    "\n",
    "## From Vector Space Model to Probabilistic Model\n",
    "\n",
    "We talked about word vectors, their dot products, and observed certain properties. How do we get those properties?\n",
    "\n",
    "\n",
    "### The Data\n",
    "\n",
    "\"Berlin is a city in East Germany\"\n",
    "\n",
    "Let's say our dataset is Wikipedia, which contains many sentences like the one above. Notice that the words in the sentence relate to one another in some way, except for somewhat meaningless words like \"is\" and \"a.\"\n",
    "\n",
    "#### Ponder the relatedness\n",
    "\n",
    "* Consider how the words \"Berlin\" and \"Germany\" might appear together in the same way that \"Paris\" and \"France\" do\n",
    "* Consider how the words \"Berlin\" and \"city\" appear together the same way \"Paris\" and \"city\" do\n",
    "\n",
    "### The Setup\n",
    "\n",
    "Let's call \"Berlin is a city in East Germany\" the current context.\n",
    "\n",
    "Let's call the input word \"city\" because it is in the center of the context, and let's call it the \"center\" word for additional clarity.\n",
    "\n",
    "\"Berlin\" \"is\" \"a\" \"in\" \"East\" \"Germany\" are all neighbor words.\n",
    "\n",
    "For every neighbor word, we have an input-output training pair, like so:\n",
    "\n",
    "\"city\" -> \"Berlin\"\n",
    "\n",
    "\"city\" -> \"is\"\n",
    "\n",
    "\"city\" -> \"a\"\n",
    "\n",
    "\"city\" -> \"in\"\n",
    "\n",
    "\"city\" -> \"East\"\n",
    "\n",
    "\"city\" -> \"Germany\"\n",
    "\n",
    "### The Objective\n",
    "\n",
    "The objective function is the maximize the probability of the output word given the input word for every input-output training pair.\n",
    "\n",
    "**Formally**\n",
    "$o,c$ is a training pair\n",
    "\n",
    "$ \\forall {o,c} $ maximize:\n",
    "\n",
    "$$ p(o | c) $$\n",
    "\n",
    "#### Conditional Probability\n",
    "\n",
    "Recall from probability theory that the conditional probability\n",
    "\n",
    "$$ p(o | c) = \\frac {p(o, c)}{p(c)} $$\n",
    "\n",
    "Notice that the numerator is the joint probability. What happens when you increase the joint probability and hold $p(c)$ fixed? The conditional probability also goes up.\n",
    "\n",
    "** Let's relate this back to word vectors **\n",
    "\n",
    "#### Word Vectors and Scores\n",
    "\n",
    "Recall that we had a word embedding, which we conveniently called $\\vec v_c$.\n",
    "\n",
    "Recall that the similarity between two word vectors was the dot product. I'm going to call that dot product the **score** or similarity, and call the function $s(\\cdot)$. (see appendix for more detail and characterization).\n",
    "\n",
    "**NOW** consider that we have two embedding matrices, $V$ and $U$, where $V$ is used for the \"center\" word $c$ and $U$ is used for the output word $o$.\n",
    "\n",
    "The score is defined as:\n",
    "\n",
    "$$ s(o,c) = \\vec v_c^T \\vec u_o$$\n",
    "\n",
    "What is desirable in our model?\n",
    "\n",
    "* We want the score for center word $c$ and output word $o$ to be high\n",
    "* We want the score for center word $c$ and all other words\n",
    "* We want to output probability\n",
    "\n",
    "Let's use this last point as the motivator to define the final form of the model we train.\n",
    "\n",
    "### Why does it work?\n",
    "\n",
    "Different words occur together in different contexts. When we model words in *vector space*, we make the vectors point in similar directions based on how those words occur together. Vectors with similar components point in similar directions, and have high dot products. Vectors with similar direction have similar neighbors.\n",
    "\n",
    "#### Back to probability-land\n",
    "\n",
    "Probabilities lie between 0 and 1, and a probability distribution must sum to 1. The score is any positive value, so it's not quite a probability, which is something we hand-wave.\n",
    "\n",
    "Let's say that\n",
    "\n",
    "$$ p(o,c) \\approx exp(s(o,c)) $$\n",
    "\n",
    "And that\n",
    "\n",
    "$$ p(c) \\approx \\sum_{w}{exp(s(w,c))} $$\n",
    "\n",
    "For all other $w$ words in the vocabulary, including $o$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$ p(o|c) \\approx \\frac{exp(s(o,c))}{\\sum_w{exp(s(w,c))}} = \\frac{\\exp(\\vec v_c^T \\vec u_o)}{\\sum_w{\\exp(\\vec v_c^T \\vec u_w)}} $$\n",
    "\n",
    "What a handful!\n",
    "\n",
    "Put another way, we want to make the probability high for the output word $o$ and low for all other $w$ words, which amounts to making the score high for $o$ and low for all other $w$.\n",
    "\n",
    "In the next section, we will talk about how we maximize that term.\n",
    "\n",
    "## Answers\n",
    "\n",
    "What is Word2Vec trained on?\n",
    "\n",
    "** Word2Vec is trained on input-output pairs of words, where the output word is a neighbor of the input word in the training corpus **\n",
    "\n",
    "What is the objective function of Word2Vec?\n",
    "\n",
    "** Word2Vec maximizes the conditional probability of the output word given the input word. This is equivalent to maximizing the joint probability of seeing the input and output word together. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We have our model:\n",
    "\n",
    "$$ \\frac{\\exp(\\vec v_c^T \\vec u_o)}{\\sum_w{\\exp(\\vec v_c^T \\vec u_w)}} $$\n",
    "\n",
    "There are not one, but **two** sets of word vectors to optimize. When we're done, we can combine them in some arbitrary way (let's say sum). As in turns out, these vectors will be learning pretty similar things, and it was a matter of experimentation that the creators of word2vec ended up with 2 sets.\n",
    "\n",
    "The word vectors to update are $v_c$ for the center word and $U$ for all possible output words, including the \"correct\" output word $o$.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We have no concept or preference of how these word vectors might look like, so we rely entirely on the data. Let's just initialize these vectors to a bunch of random small numbers.\n",
    "\n",
    "### Gradients\n",
    "\n",
    "There are entire courses on optimization of neural networks; instead of going over them in any amount of detail and not doing them justice, I'm going to tell you that they are black boxes and just give you the \"gradients\" that you will be using.\n",
    "\n",
    "The gradients for each word vector tell you how to update them for every example you see. This process is called Stochastic Gradient Descent, and the gradients come from applying the chain rule of calculus in a neural network using the process of Backpropagataion.\n",
    "\n",
    "Something to pay attention to:\n",
    "Signs are important! I subtract the gradient. When I add a gradient to a vector, I'm pushing that vector towards the gradient. (show diagram)\n",
    "\n",
    "### Center word\n",
    "\n",
    "Let's look at the gradient for $v_c$, which we call $\\frac{\\partial}{\\partial v_c}$, and see if we can gain some intuition about it.\n",
    "\n",
    "#### Gradient Update\n",
    "\n",
    "You are given the center word and need to update the word vector upon seeing a new example. The example tells you that the center word and output word should have a higher score together. It also implies that the center word and all other words should have a lower score. Let's see how that plays out in the math.\n",
    "\n",
    "Let's use Berlin-city and Germany-city as an example. By moving the vectors of Berlin and city closer together, as well as Germany and city, we get vectors that are similar\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial v_c} = \\sum_w^W p(w|c) \\vec u_w - \\vec u_o = \\sum_{w \\ne o}^{W} p(w|c) * u_{i,j}  -  (p(o|c) - 1) u_o $$\n",
    "\n",
    "$$ v_c = v_c - \\alpha * \\frac{\\partial}{\\partial v_c} $$\n",
    "\n",
    "#### Vector Space Intuition\n",
    "\n",
    "What is this update doing? It's adding a small multiple $\\alpha$ of the gradient, which we can further break down.\n",
    "\n",
    "First, it's adding a small bit of $u_o$. What happens when you add $u_o$ to $v_c$? Let's visualize the new $v_c$ when adding $u_o$:\n",
    "\n",
    "** VISUALIZE **\n",
    "\n",
    "Notice that $v_c$ moves towards $u_o$ - it becomes more similar to $u_o$. This makes sense, because we're trying to increase their dot product and therefore their similarity.\n",
    "\n",
    "Now, let's add the sum of all $u_w$ vectors. Individually, what is happening? You're adding the negative of every $u_w$ vector, which we can visualize:\n",
    "\n",
    "** VISUALIZE **\n",
    "\n",
    "Notice that $v_c$ moves away from every $u_w$, becoming less similar. This is because we're trying to decrease their dot product.\n",
    "\n",
    "### Output words\n",
    "\n",
    "Let's look at the gradient for $u_o$ and all other $u_w$s and see if we can gain some intution:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial u_w} =\n",
    "    \\begin{cases}\n",
    "    \\hat y_w \\cdot v_c - v_c = v_c(\\hat y_w - 1) &\\text{if } w = o \\\\[2ex]\n",
    "    \\hat y_w \\cdot v_c = v_c * \\hat y_w &\\text{if } w \\ne o\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$ u_w = u_w - \\alpha * \\frac{\\partial}{\\partial u_w} $$\n",
    "\n",
    "#### Vector Space Intuition\n",
    "\n",
    "What is this update doing?\n",
    "\n",
    "For the correct output word $u_o$, it's adding a small multiple $\\alpha$ times the center word vector $v_c$, scaled by the probability error (how much it is less than 1). It's moving $u_o$ slightly towards $v_c$.\n",
    "\n",
    "For all other output words $u_w$, it's subtracting a small multiple times the center word vector $v_c$, scaled by the predicted probability (how much it is greater than 0). It's moving $u_w$ slightly away from $v_c$.\n",
    "\n",
    "\n",
    "### Illusion, Explained\n",
    "\n",
    "When you encounter a pair of words, they are treated as related and their word vectors move towards each other. Various pairs of words push and pull, and they start to line up in ways that we perceive to be their relationships.\n",
    "\n",
    "This is merely an illusion of the words being neighbors in different contexts. From an initial random configuration, word vectors move towards each other along the axis in which they are currently aligned. Similar pairs of co-occurrences, i.e. Paris-France and Berlin-Germany, end up resulting in similar movements.\n",
    "\n",
    "That's how you get this resulting representation. I'd say it's a pretty good one!\n",
    "\n",
    "### Answer\n",
    "\n",
    "How does Word2Vec update individual word vectors when training?\n",
    "\n",
    "** Word2Vec moves the vectors for the center and output word towards each other by adding a small multiple of each one to the other **\n",
    "\n",
    "Why do those updates lead to vector space properties discussed above?\n",
    "\n",
    "** Pairs of neighbors words often occur together, leading to movement in vector space that is parallel. This parallel movement results in arrangements that are where related word vector pairs form diamond shapes. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications & Appendix\n",
    "\n",
    "## Conceptual Application\n",
    "\n",
    "When you have complicated models, you have more transformations between dense representations.\n",
    "However, the intuition remains the same; you're dealing with dot products and similarities (negative distances).\n",
    "\n",
    "As you train models with either embeddings or hidden layers, keep in mind that you're always either trying to increase or decrease dot products between your weights and your examples. In a sense, you are memorizing your dataset in a continuous way. With enough weights, it's no surprise that you can overfit!\n",
    "\n",
    "## Usage\n",
    "\n",
    "In a deep learning language model, you can \"pretrain\" your word vectors using Word2Vec to use as inputs into your first layer of a recurrent neural network. You can also keep the weights in your network and \"finetune\" them by backpropagating into them.\n",
    "\n",
    "You can also use the $V$ matrix as a \"de-embedding\" matrix for going from the final hidden state (which could be recurrent) of your model to scores over your vocabulary, which you would run a softmax over.\n",
    "\n",
    "\n",
    "## Appendix\n",
    "\n",
    "### Unnormalized Log Probability\n",
    "\n",
    "$ p(o,c) $ is the joint probability, and it is proportional to:\n",
    "$ \\vec v_c^T \\vec u_o$, which is the dot product.\n",
    "\n",
    "If we exponentiate the dot product\n",
    "\n",
    "$exp(\\vec v_c^T \\vec u_o)$\n",
    "\n",
    "Then normalize by all exponentiated dot products over $c$\n",
    "\n",
    "$\\sum_w exp(\\vec v_c^T \\vec u_w)$\n",
    "\n",
    "That starts to look like $\\frac{p(o,c)}{p(c)}$, which is in fact the probability we were after.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "* We exponentiated the dot product\n",
    "* We normalized over all exponentiated dot products for a center word\n",
    "* In the inverse, we can say we got to the dot product from the probability by unnormalizing and taking the log.\n",
    "\n",
    "### Cross Entropy & Derivation\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Cross entropy is an error measure that represents the different between a predicted probability distribution and the true distribution.\n",
    "\n",
    "$CE(y, \\hat y) = - \\sum_i y_i log(\\hat y_i)$\n",
    "\n",
    "#### Derivation of cross entropy gradient\n",
    "\n",
    "$CE(y, \\hat y) = - \\sum_i y_i log(\\hat y_i)$\n",
    "\n",
    "$ \\hat y_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_k} CE(y, \\hat y) = \n",
    "- \\frac{\\partial}{\\partial x_k} \\sum_i y_i log(\\hat y_i)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial x_k} log(\\hat y_i)$\n",
    "\n",
    "$= - \\frac{\\partial}{\\partial x_k} log \\frac{e^{x_i}}{\\sum_j e^{x_j}} $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} \\log e^{x_i} - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{\\partial}{\\partial x_k} \\log \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{1}{\\sum_j e^{x_j}} * \\frac{\\partial}{\\partial x_k} \\sum_j e^{x_j} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{1}{\\sum_j e^{x_j}} * \\frac{\\partial}{\\partial x_k} e^{x_k} ) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\frac{e^{x_k}}{\\sum_j e^{x_j}}) $\n",
    "\n",
    "$= - ( \\frac{\\partial}{\\partial x_k} x_i - \\hat y_k) $\n",
    "\n",
    "$= \\hat y_k - \\frac{\\partial}{\\partial x_k} x_i $\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial x_k} CE(y, \\hat y) =\n",
    "    \\begin{cases}\n",
    "    \\hat y_k - 1 &\\text{if } i = k \\\\[2ex]\n",
    "    \\hat y_k - 0 &\\text{if } i \\ne k\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "OR\n",
    "\n",
    "$\\frac{\\partial}{\\partial x} CE(y, \\hat y) = \\hat y - y$\n",
    "\n",
    "### Word2Vec Derivation\n",
    "\n",
    "#### Derive gradients wrt $v_c$\n",
    "\n",
    "$J_{softmax-CE}(o, v_c, u_o) = - \\sum_i y_i \\log \\hat y_i = - \\log \\hat y_y$\n",
    "\n",
    "where $\\hat y_y$ is the predicted probability for the correct output word $y$.\n",
    "\n",
    "$\\hat y_o = p(o | c)$\n",
    "\n",
    "For an individual outer-word | inner-word pair $o, c$, the `score`, or unnormalized log probability is defined as:\n",
    "\n",
    "$z_{w,c} = u_w^T v_c$\n",
    "\n",
    "You can vectorize the `score` by thinking of it as a matrix-vector product:\n",
    "\n",
    "$z_c = U v_c$\n",
    "\n",
    "Where $U$ is a row-matrix of outer-word vectors. The `score` represents the inner-product similarity between the two words, and the softmax function will accentuate the highest scoring pair.\n",
    "\n",
    "$softmax(z_{o,c})$\n",
    "\n",
    "$= \\frac {exp(z_{o,c})}{\\sum_{w=1}{W} exp(z_{w,c})}$\n",
    "\n",
    "$\\approx \\max_w z_c$\n",
    "\n",
    "Since $y_i = 0 \\forall i \\ne k$\n",
    "\n",
    "On individual elements:\n",
    "\n",
    "$\\frac {\\partial}{\\partial v_c} - \\log \\hat y$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} - \\log softmax(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} - \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial v_c} \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}]$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial v_c} \\log exp(z_{o,c}) - \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W (u_w^T v_c)]$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial v_c} \\log exp(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\frac {\\partial}{\\partial v_c} \\sum_{x=1}^W \\exp(u_x^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)}  \\frac {\\partial}{\\partial v_c} \\exp(u_x^T v_c) - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\exp(u_x^T v_c) \\cdot u_x - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\frac {\\exp(u_x^T v_c)}{\\sum_{w=1}^W \\exp(u_w^T v_c)}  \\cdot u_x - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W p(x|c) \\cdot u_x - \\frac {\\partial}{\\partial v_c} z_{o,c}$\n",
    "\n",
    "$= \\sum_{x=1}^W \\hat y_x \\cdot u_x - \\frac {\\partial}{\\partial v_c} u_o^T v_c$\n",
    "\n",
    "$= \\sum_{x=1}^W \\hat y_x \\cdot u_x - u_o$\n",
    "\n",
    "Equivalently, on vectors:\n",
    "\n",
    "$\\frac{\\partial}{\\partial v_c} J_{softmax-CE}(o, v_c, U)$\n",
    "\n",
    "$= \\frac{\\partial J}{\\partial z_c} \\frac{\\partial z_c}{\\partial v_c}$\n",
    "\n",
    "Because $\\frac{\\partial CE(y, \\hat y)}{\\partial \\theta} = \\hat y - y$ from above:\n",
    "\n",
    "$= (\\hat y - y) ^T \\frac{\\partial z_c}{\\partial v_c}$ (to match dimensions)\n",
    "\n",
    "Because $z_c = U v_c$ and the gradient is the transpose of the Jacobian:\n",
    "\n",
    "$= ((\\hat y - y) U)^T$\n",
    "\n",
    "$= U^T (\\hat y - y)$\n",
    "\n",
    "#### Derive gradients wrt all $u_w$ including $u_o$\n",
    "\n",
    "As before:\n",
    "\n",
    "$J_{softmax-CE}(c, v_c, u_o) = - \\sum_i y_i \\log \\hat y = - \\log \\hat y$\n",
    "\n",
    "Since $y_i = 0 \\forall i \\ne k$\n",
    "\n",
    "$\\hat y_o = p(o | c)$\n",
    "\n",
    "$z_{w,c} = u_w^T v_c$\n",
    "\n",
    "$softmax(z_{o,c}) = \\frac {exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}$\n",
    "\n",
    "On individual elements:\n",
    "\n",
    "$\\frac {\\partial}{\\partial u_w} - \\log \\hat y$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} - \\log softmax(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} - \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial u_w} \\log \\frac {\\exp(z_{o,c})}{\\sum_{w=1}{W} (u_w^T v_c)}]$\n",
    "\n",
    "$= - [\\frac {\\partial}{\\partial u_w} \\log exp(z_{o,c}) - \\frac {\\partial}{\\partial v_c} \\log \\sum_{w=1}^W (u_w^T v_c)]$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial u_w} \\log exp(z_{o,c})$\n",
    "\n",
    "$= \\frac {\\partial}{\\partial u_w} \\log \\sum_{w=1}^W \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\frac {\\partial}{\\partial u_w} \\sum_{x=1}^W \\exp(u_x^T v_c) - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "The sum goes away since the derivative is 0 when $x \\ne w$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\frac {\\partial}{\\partial u_w} \\exp(u_w^T v_c) - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\exp(u_w^T v_c) \\frac {\\partial}{\\partial u_w} u_w^T v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {1}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\exp(u_w^T v_c) \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\frac {\\exp(u_w^T v_c)}{\\sum_{w=1}^W \\exp(u_w^T v_c)} \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= p(w | c) \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\hat y_w \\cdot v_c - \\frac {\\partial}{\\partial u_w} z_{o,c}$\n",
    "\n",
    "$= \\hat y_w \\cdot v_c - \\frac {\\partial}{\\partial u_w} u_o^T v_c$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial u_w} CE(y, \\hat y) =\n",
    "    \\begin{cases}\n",
    "    \\hat y_w \\cdot v_c - v_c = v_c(\\hat y_w - 1) &\\text{if } w = o \\\\[2ex]\n",
    "    \\hat y_w \\cdot v_c = v_c * \\hat y_w &\\text{if } w \\ne o\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "### Euclidian Word Distance\n",
    "\n",
    "L2 distance between 2 word vectors \n",
    "$$ d^2 = (x - z)^2 = x^2 + z^2 - 2xz $$\n",
    "\n",
    "Setting them to normal vectors\n",
    "$$ d^2 = 2 - 2xz $$\n",
    "\n",
    "Then exponentiating for unnormalized probability ->\n",
    "\n",
    "$$ exp(-d^2) $$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
