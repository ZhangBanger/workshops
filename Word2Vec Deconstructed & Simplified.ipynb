{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "### Synonyms\n",
    "\n",
    "$$ \\vec {happiness} \\approx \\vec {joy} $$\n",
    "\n",
    "$$ \\vec {dog} \\approx \\vec {puppy} $$\n",
    "\n",
    "And also this?\n",
    "\n",
    "$$ \\vec {one} \\approx \\vec {two} $$\n",
    "\n",
    "### Analogies\n",
    "\n",
    "You can do arithmetic and get analogies!\n",
    "\n",
    "$$ \\vec {king} - \\vec {man} + \\vec {woman} \\approx \\vec {queen} $$\n",
    "\n",
    "$$ \\vec {Berlin} - \\vec {Germany} + \\vec {France} \\approx \\vec {Paris} $$\n",
    "\n",
    "## Magic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "* What is an embedding?\n",
    "* What is a good representation?\n",
    "* Why are embeddings considered good representations?\n",
    "\n",
    "### One-Hot\n",
    "\n",
    "$$ \\vec dog = \\begin{bmatrix} 0 & 0 & ... & 1  & ... & 0  \\end{bmatrix} $$\n",
    "\n",
    "~ 1M dimensions\n",
    "\n",
    "### Dense Embedding\n",
    "\n",
    "$$ \\vec dog = \\begin{bmatrix} 2.5 & -1.6 & ... & 3.2 \\end{bmatrix} $$\n",
    "\n",
    "~ 100-300 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model\n",
    "\n",
    "### Vector Sum\n",
    "\n",
    "The sum of vectors $v$ and $u$ is defined as addition of their individual elements.\n",
    "\n",
    "$$ w_i = v_i + u_i $$\n",
    "\n",
    "The resulting vector is the equivalent of adding drawing the tail of $u$ from the head of $v$:\n",
    "\n",
    "### Dot Product -> Similarity\n",
    "\n",
    "The dot product of vectors $\\vec v$ and $\\vec u$ is defined as the sum of the product of their individual elements.\n",
    "\n",
    "$$ \\vec v^T \\vec u = \\sum_i v_i * u_i $$\n",
    "\n",
    "### Good Representations\n",
    "\n",
    "What is a good representation?\n",
    "\n",
    "** Good representations take advantage of prior knowledge to make a downstream machine learning task easier **\n",
    "\n",
    "Why are dense embeddings good representations?\n",
    "\n",
    "** Dense embeddings represent words based on their relatedness, so that a downstream task can make use of such relatedness without having to learn it on its own. The dot product of two related words is high, and the dot product of two unrelated words is low. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model\n",
    "\n",
    "## Working Example\n",
    "\n",
    "\"Berlin is a **city** in East Germany\"\n",
    "\n",
    "## Probabilistic Model\n",
    "\n",
    "With input of \"city\" predict words that surround it\n",
    "\n",
    "Berlin -> high probability\n",
    "Germany -> high probability\n",
    "potato -> low probability\n",
    "banana -> low probability\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "$$ p({Berlin} | {city}) = \\frac{p({Berlin}, {city})}{p(city)} $$\n",
    "\n",
    "## Softmax\n",
    "\n",
    "$$ \\frac {exp(score({ThisWord}, city))}{\\sum_{word}^{AllWords} exp(score(word, city)} $$\n",
    "\n",
    "### Return of dot products!\n",
    "\n",
    "$$ \\frac{exp(\\vec {Berlin} ^T \\vec {city})}{\\sum_{word} exp(\\vec {word} ^T \\vec {city})} $$\n",
    "\n",
    "## What are good dot products?\n",
    "\n",
    "Berlin and city should have high dot products\n",
    "Germany and city should have high dot products\n",
    "potato and city should have low dot products\n",
    "banana and city should have low dot products\n",
    "\n",
    "## Why?\n",
    "\n",
    "Berlin and city occur often together, and therefore we consider them more related\n",
    "banana and city don't occur often together, and therefore we consider them less related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Start Random\n",
    "\n",
    "## Single example\n",
    "\n",
    "\"Berlin is a **city** in East Germany\"\n",
    "\n",
    "### How do Berlin and city move?\n",
    "\n",
    "### How do Germany and city move?\n",
    "\n",
    "### How do banana and city move?\n",
    "\n",
    "## Other examples\n",
    "\n",
    "\"Berlin is in the **country** of Germany\"\n",
    "\n",
    "\"Paris is a **city** in France\"\n",
    "\n",
    "## Illusion, Explained\n",
    "\n",
    "How does Word2Vec update individual word vectors when training?\n",
    "\n",
    "** Word2Vec moves the vectors for the center and output word towards each other by adding a small multiple of each one to the other **\n",
    "\n",
    "Why do those updates lead to vector space properties discussed above?\n",
    "\n",
    "** Words with similar neighbors move together in parallel. This parallel movement results in arrangements that are where related word vector pairs form diamond shapes. **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
